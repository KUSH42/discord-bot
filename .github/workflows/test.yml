name: Comprehensive Test Suite

# Add permissions for GitHub Actions
permissions:
  contents: write
  pull-requests: write
  issues: write
  packages: write

on:
  push:
    branches: 
      - 'master'      # Production branch
      - 'develop'     # Main development branch  
      - 'dev'         # Development branch
  pull_request:
    branches: 
      - '*'           # PRs targeting any branch
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - security

env:
  NODE_VERSION: '20'
  CACHE_KEY_PREFIX: 'discord-bot-test'
  GH_TOKEN: ${{ github.token }}

jobs:
  # Pre-flight checks
  pre-flight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.check.outputs.should-run }}
      test-matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if tests should run
        id: check
        run: |
          set -e
          
          # Validate inputs for security
          if [[ "${{ github.event_name }}" == "push" ]] && [[ -n "${{ github.event.before }}" ]]; then
            if ! [[ "${{ github.event.before }}" =~ ^[a-f0-9]{7,40}$ ]]; then
              echo "Invalid before SHA format, running tests for safety"
              echo "should-run=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Always run tests on PRs
            echo "should-run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "push" ]]; then
            # Run tests if relevant files changed
            if git diff --name-only "${{ github.event.before }}" "${{ github.sha }}" | grep -E '\.(js|json|yml|yaml)$' >/dev/null 2>&1; then
              echo "should-run=true" >> $GITHUB_OUTPUT
            else
              echo "should-run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate test matrix
        id: matrix
        run: |
          if [[ "${{ github.event.inputs.test_type }}" == "unit" ]]; then
            echo 'matrix={"test-type": ["unit"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "integration" ]]; then
            echo 'matrix={"test-type": ["integration"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "e2e" ]]; then
            echo 'matrix={"test-type": ["e2e"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "performance" ]]; then
            echo 'matrix={"test-type": ["performance"]}' >> $GITHUB_OUTPUT
          elif [[ "${{ github.event.inputs.test_type }}" == "security" ]]; then
            echo 'matrix={"test-type": ["security"]}' >> $GITHUB_OUTPUT
          else
            echo 'matrix={"test-type": ["unit", "integration", "e2e", "performance", "security"]}' >> $GITHUB_OUTPUT
          fi

  # Code quality and linting
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run ESLint
        run: |
          if [ -f .eslintrc.js ] || [ -f .eslintrc.json ]; then
            npm run lint || npx eslint . --ext .js --format=compact
          else
            echo "No ESLint configuration found, skipping lint check"
          fi

      - name: Run Prettier check
        run: |
          if [ -f .prettierrc ] || [ -f .prettierrc.json ]; then
            npx prettier --check "**/*.{js,json,md,yml,yaml}"
          else
            echo "No Prettier configuration found, skipping format check"
          fi

      - name: Check for security vulnerabilities
        run: npm audit --audit-level=moderate

      - name: Validate package.json
        run: npm ls --depth=0

  # Unit tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'unit')
    strategy:
      matrix:
        node-version: ['18', '20']
        shard: [1, 2, 3, 4]
      fail-fast: false
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: |
          mkdir -p coverage/unit
          set -e
          set -o pipefail
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=2048" \
          npm run test:unit -- \
            --shard=${{ matrix.shard }}/4 \
            --coverage \
            --coverageDirectory=coverage/unit \
            --coverageThreshold='{}' \
            --coverageReporters=text --coverageReporters=lcov --coverageReporters=clover \
            --maxWorkers=2 \
            --forceExit \
            --detectOpenHandles 2>&1 | tee coverage/unit/test-output-node${{ matrix.node-version }}-${{ matrix.shard }}.log

      - name: Upload unit test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/unit/lcov.info
          flags: unit-node${{ matrix.node-version }}-shard${{ matrix.shard }}
          name: unit-tests-node${{ matrix.node-version }}-shard${{ matrix.shard }}
          fail_ci_if_error: false
          plugins: ''

      - name: Store unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-node${{ matrix.node-version }}-shard${{ matrix.shard }}
          path: |
            coverage/unit/
            junit.xml
          retention-days: 7

  # Integration tests
  build-and-push-docker:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [pre-flight]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'integration')
    outputs:
      image_name: ghcr.io/${{ steps.string.outputs.repo_lc }}
      image_tag: ${{ steps.final-image.outputs.image_tag_final }}
      image_digest: ${{ steps.build.outputs.digest || steps.force-build.outputs.digest }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set repo to lowercase
        id: string
        run: echo "repo_lc=$(echo '${{ github.repository }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_OUTPUT

      - name: Check if Docker build is needed
        id: docker-changes
        run: |
          # For PRs and first push, always build
          if [[ "${{ github.event_name }}" == "pull_request" ]] || [[ "${{ github.event.before }}" == "0000000000000000000000000000000000000000" ]]; then
            echo "docker_changed=true" >> $GITHUB_OUTPUT
            echo "Building Docker image for PR or initial push"
            exit 0
          fi
          
          # Check if Docker-related files changed since last commit
          if git diff --name-only "${{ github.event.before }}" "${{ github.sha }}" | grep -E '^(Dockerfile|\.dockerignore|package\.json|package-lock\.json|src/|tests/)$'; then
            echo "docker_changed=true" >> $GITHUB_OUTPUT
            echo "Docker-related files changed, will build new image"
          else
            echo "docker_changed=false" >> $GITHUB_OUTPUT
            echo "No Docker-related changes detected, will try to use cached image"
          fi

      - name: Generate Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ steps.string.outputs.repo_lc }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-,enable={{is_default_branch}}
            type=sha
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.title=Discord YouTube Bot
            org.opencontainers.image.description=Integration test environment
            org.opencontainers.image.vendor=discord-youtube-bot

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            network=host

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        id: build
        if: steps.docker-changes.outputs.docker_changed == 'true'
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64
          target: test-runner
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1

      - name: Use existing image if no changes
        id: existing-image
        if: steps.docker-changes.outputs.docker_changed == 'false'
        run: |
          # Try to pull the latest image to use as fallback
          LATEST_TAG="ghcr.io/${{ steps.string.outputs.repo_lc }}:${{ github.ref_name }}-${{ github.sha }}"
          FALLBACK_TAG="ghcr.io/${{ steps.string.outputs.repo_lc }}:latest"
          
          # Check if the exact SHA-based image exists
          if docker manifest inspect "$LATEST_TAG" >/dev/null 2>&1; then
            echo "Using existing image: $LATEST_TAG"
            echo "image_tag_raw=$LATEST_TAG" >> $GITHUB_OUTPUT
          elif docker manifest inspect "$FALLBACK_TAG" >/dev/null 2>&1; then
            echo "Using fallback image: $FALLBACK_TAG"
            echo "image_tag_raw=$FALLBACK_TAG" >> $GITHUB_OUTPUT
          else
            echo "No existing image found, forcing rebuild"
            echo "force_rebuild=true" >> $GITHUB_OUTPUT
          fi

      - name: Force rebuild if no cached image available
        id: force-build
        if: steps.docker-changes.outputs.docker_changed == 'false' && steps.existing-image.outputs.force_rebuild == 'true'
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64
          target: test-runner
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1

      - name: Set final image tag
        id: final-image
        run: |
          FINAL_TAG=""
          if [[ "${{ steps.docker-changes.outputs.docker_changed }}" == "true" ]] || [[ "${{ steps.existing-image.outputs.force_rebuild }}" == "true" ]]; then
            # A new image was built (or force-rebuilt). Use the first tag from the metadata action.
            FINAL_TAG=$(echo "${{ steps.meta.outputs.tags }}" | head -n 1)
          else
            # Use the tag for the existing image.
            FINAL_TAG="${{ steps.existing-image.outputs.image_tag_raw }}"
          fi
          echo "image_tag_final=${FINAL_TAG}" >> $GITHUB_OUTPUT

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    container:
      image: ${{ needs.build-and-push-docker.outputs.image_tag }}
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    needs: [pre-flight, lint, build-and-push-docker]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'integration')
    services:
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Set up test environment
        run: |
          cat > .env.test << 'EOF'
          NODE_ENV=test
          DISCORD_BOT_TOKEN=test-token-placeholder
          YOUTUBE_API_KEY=test-api-key-placeholder
          YOUTUBE_CHANNEL_ID=UCtest123456789012345678
          DISCORD_YOUTUBE_CHANNEL_ID=123456789012345678
          PSH_CALLBACK_URL=https://test.example.com/webhook
          X_USER_HANDLE=testuser
          DISCORD_X_POSTS_CHANNEL_ID=234567890123456789
          DISCORD_X_REPLIES_CHANNEL_ID=345678901234567890
          DISCORD_X_QUOTES_CHANNEL_ID=456789012345678901
          DISCORD_X_RETWEETS_CHANNEL_ID=567890123456789012
          TWITTER_USERNAME=testuser
          TWITTER_PASSWORD=testpassword
          DISCORD_BOT_SUPPORT_LOG_CHANNEL=678901234567890123
          PSH_SECRET=test-webhook-secret-for-integration-testing
          REDIS_URL=redis://localhost:6379
          EOF

      - name: Run integration tests
        env:
          NODE_ENV: test
        run: |
          mkdir -p coverage/integration
          set -e
          set -o pipefail
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=2048" \
          npm run test:integration -- \
            --coverage \
            --coverageDirectory=coverage/integration \
            --coverageReporters=text --coverageReporters=lcov \
            --maxWorkers=1 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=${{ vars.TEST_TIMEOUT || '30000' }} 2>&1 | tee coverage/integration/test-output.log

      - name: Upload integration test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/integration/lcov.info
          flags: integration
          name: integration-tests
          fail_ci_if_error: false
          plugins: ''

      - name: Store integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            coverage/integration/
            logs/
          retention-days: 7

  # End-to-end tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'e2e')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Set up E2E test environment
        run: |
          cat > .env.e2e << 'EOF'
          NODE_ENV=test
          LOG_LEVEL=error
          DISCORD_BOT_TOKEN=test-bot-token-e2e-placeholder
          YOUTUBE_API_KEY=test-youtube-api-key-placeholder
          YOUTUBE_CHANNEL_ID=UCtest123456789012345678
          DISCORD_YOUTUBE_CHANNEL_ID=123456789012345678
          PSH_CALLBACK_URL=https://test-e2e.example.com/webhook
          PSH_PORT=3001
          X_USER_HANDLE=testuser
          DISCORD_X_POSTS_CHANNEL_ID=234567890123456789
          DISCORD_X_REPLIES_CHANNEL_ID=345678901234567890
          DISCORD_X_QUOTES_CHANNEL_ID=456789012345678901
          DISCORD_X_RETWEETS_CHANNEL_ID=567890123456789012
          TWITTER_USERNAME=testuser
          TWITTER_PASSWORD=testpassword
          DISCORD_BOT_SUPPORT_LOG_CHANNEL=678901234567890123
          PSH_SECRET=e2e-test-webhook-secret
          ANNOUNCEMENT_ENABLED=true
          X_VX_TWITTER_CONVERSION=false
          ALLOWED_USER_IDS=123456789012345678,987654321098765432
          EOF

      - name: Run E2E tests
        env:
          NODE_ENV: test
        run: |
          mkdir -p test-results/e2e
          set -e
          set -o pipefail
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=2048" \
          npx jest --config jest.e2e.config.js \
            --maxWorkers=1 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=${{ vars.E2E_TIMEOUT || '60000' }} 2>&1 | tee test-results/e2e/test-output.log

      - name: Store E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            logs/
            screenshots/
            .env.e2e
            test-results/e2e/
          retention-days: 7
          if-no-files-found: warn

  # Performance tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'performance')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run performance tests
        run: |
          mkdir -p coverage/performance
          set -e
          set -o pipefail
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=4096" \
          npm run test:performance -- \
            --coverage \
            --coverageDirectory=coverage/performance \
            --coverageReporters=text --coverageReporters=lcov \
            --maxWorkers=1 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=${{ vars.PERFORMANCE_TIMEOUT || '120000' }} 2>&1 | tee coverage/performance/test-output.log

      - name: Generate performance report
        run: |
          mkdir -p reports
          echo "# Performance Test Report" > reports/performance.md
          echo "Generated on: $(date)" >> reports/performance.md
          echo "Node.js version: $(node --version)" >> reports/performance.md
          echo "Platform: ${{ runner.os }}" >> reports/performance.md
          echo "" >> reports/performance.md
          
          # Extract performance metrics from test output
          echo "## Performance Metrics" >> reports/performance.md
          echo "" >> reports/performance.md
          
          # Parse Jest output for timing information
          if [ -f coverage/performance/test-output.log ]; then
            echo "### Test Execution Times" >> reports/performance.md
            grep -E "Test completed in [0-9]+\.[0-9]+ms" coverage/performance/test-output.log | head -10 >> reports/performance.md || echo "No timing data found" >> reports/performance.md
            echo "" >> reports/performance.md
            
            echo "### Throughput Metrics" >> reports/performance.md
            grep -E "(messages per second|URLs per second|webhooks per second)" coverage/performance/test-output.log | head -10 >> reports/performance.md || echo "No throughput data found" >> reports/performance.md
            echo "" >> reports/performance.md
          fi
          
          # Add system performance info
          echo "### System Performance" >> reports/performance.md
          echo "- **CPU Info**: $(nproc) cores" >> reports/performance.md
          echo "- **Memory**: $(free -h | grep '^Mem:' | awk '{print $2}')" >> reports/performance.md
          echo "- **Test Duration**: $(date -d @$SECONDS -u '+%M:%S')" >> reports/performance.md
          echo "" >> reports/performance.md
          
          # Create metrics summary JSON
          cat > performance-metrics.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "node_version": "$(node --version)",
            "platform": "${{ runner.os }}",
            "cpu_cores": $(nproc),
            "memory_total": "$(free -h | grep '^Mem:' | awk '{print $2}')",
            "test_duration_seconds": $SECONDS
          }
          EOF

      - name: Upload performance test coverage
        uses: codecov/codecov-action@v4
        with:
          file: coverage/performance/lcov.info
          flags: performance
          name: performance-tests
          fail_ci_if_error: false
          plugins: ''

      - name: Store performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            coverage/performance/
            reports/
            performance-metrics.json
          retention-days: 30

  # Security tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: [pre-flight, lint]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && contains(fromJson(needs.pre-flight.outputs.test-matrix).test-type, 'security')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run security tests
        run: |
          mkdir -p test-results/security
          set -e
          set -o pipefail
          NODE_OPTIONS="--experimental-vm-modules --max-old-space-size=2048" \
          npx jest --config jest.security.config.js \
            --maxWorkers=2 \
            --forceExit \
            --detectOpenHandles \
            --testTimeout=${{ vars.SECURITY_TIMEOUT || '45000' }} 2>&1 | tee test-results/security/test-output.log

      - name: Run security audit
        run: |
          npm audit --audit-level=moderate --json > security-audit.json || true
          
          # Check for high/critical vulnerabilities
          HIGH_VULNS=$(cat security-audit.json | jq '.metadata.vulnerabilities.high // 0')
          CRITICAL_VULNS=$(cat security-audit.json | jq '.metadata.vulnerabilities.critical // 0')
          
          echo "High vulnerabilities: $HIGH_VULNS"
          echo "Critical vulnerabilities: $CRITICAL_VULNS"
          
          if [ "$CRITICAL_VULNS" -gt 0 ]; then
            echo "❌ Critical vulnerabilities found!"
            exit 1
          elif [ "$HIGH_VULNS" -gt 0 ]; then
            echo "⚠️ High vulnerabilities found!"
          fi

      - name: Check for secrets
        run: |
          # Simple check for potential secrets in code
          echo "Checking for potential secrets..."
          
          # Check for potential API keys, tokens, etc.
          SECRET_PATTERNS=(
            "api[_-]?key"
            "secret[_-]?key"
            "access[_-]?token"
            "auth[_-]?token"
            "discord[_-]?token"
            "password"
          )
          
          for pattern in "${SECRET_PATTERNS[@]}"; do
            if git grep -i "$pattern" -- '*.js' '*.json' | grep -v test | grep -v mock | grep -v example; then
              echo "⚠️ Potential secret found: $pattern"
            fi
          done

      - name: Store security test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-test-results
          path: |
            security-audit.json
            security-report.json
            test-results/security/
          retention-days: 30

  # Aggregate results and report
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    env:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-tests]
    if: always() && needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install coverage tools
        run: |
          npm install -g nyc

      - name: Download Node 18 unit test coverage (all shards)
        uses: actions/download-artifact@v4
        continue-on-error: ${{ needs.unit-tests.result == 'skipped' }}
        with:
          pattern: unit-test-results-node18-*
          merge-multiple: true
          path: test-results/unit/node18/

      - name: Download Node 20 unit test coverage (all shards)
        uses: actions/download-artifact@v4
        continue-on-error: ${{ needs.unit-tests.result == 'skipped' }}
        with:
          pattern: unit-test-results-node20-*
          merge-multiple: true
          path: test-results/unit/node20/

      - name: Download integration test results
        uses: actions/download-artifact@v4
        with:
          name: integration-test-results
          path: test-results/integration/

      - name: Download e2e test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: e2e-test-results
          path: test-results/e2e/

      - name: Download performance test coverage
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: performance-test-results
          path: test-results/performance/

      - name: Download security test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: security-test-results
          path: test-results/security/

      - name: Merge coverage reports (avoid triple-counting)
        id: coverage
        run: |
          mkdir -p coverage/merged
          
          # Find coverage files from test types that generate coverage (unit, integration, performance)
          echo "Collecting coverage from test types that generate source code coverage..."
          
          # First, ensure we have proper directory structure
          ls -laR test-results/ || echo "No test-results directory found"
          
          # Find all lcov.info files but prioritize Node 18 over Node 20 to avoid duplicates
          COVERAGE_FILES=()
          
          # Add Node 18 unit coverage (primary)
          if [ -f "test-results/unit/node18/coverage/unit/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/unit/node18/coverage/unit/lcov.info")
            echo "Found Node 18 unit coverage"
          fi
          
          # Add integration coverage if available
          if [ -f "test-results/integration/coverage/integration/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/integration/coverage/integration/lcov.info")
            echo "Found integration coverage"
          fi
          
          # Add performance coverage if available
          if [ -f "test-results/performance/coverage/performance/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/performance/coverage/performance/lcov.info")
            echo "Found performance coverage"
          fi
          
          # Add e2e coverage if available (not typically expected but check anyway)
          if [ -f "test-results/e2e/coverage/e2e/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/e2e/coverage/e2e/lcov.info")
            echo "Found e2e coverage"
          fi
          
          # Add security coverage if available (not typically expected but check anyway)
          if [ -f "test-results/security/coverage/security/lcov.info" ]; then
            COVERAGE_FILES+=("test-results/security/coverage/security/lcov.info")
            echo "Found security coverage"
          fi
          
          if [ ${#COVERAGE_FILES[@]} -gt 0 ]; then
            echo "Merging the following coverage files:"
            printf '%s\n' "${COVERAGE_FILES[@]}"
            
            # Debug: Check the source lcov files before merging
            echo "Debugging source coverage files:"
            for file in "${COVERAGE_FILES[@]}"; do
              echo "File: $file"
              echo "  Size: $(wc -l < "$file") lines"
              echo "  Sample content:"
              head -10 "$file"
              echo "  Coverage totals:"
              grep -E "^(LF|LH|BRF|BRH|FNF|FNH):" "$file" | head -5
              echo "---"
            done
            
            # Validate coverage merge script exists
            if [[ ! -f "scripts/merge-coverage.py" ]]; then
              echo "ERROR: Coverage merge script not found, falling back to concatenation"
              cat "${COVERAGE_FILES[@]}" > coverage/merged/lcov.info
            else
              # Validate script syntax
              if ! python3 -m py_compile scripts/merge-coverage.py; then
                echo "ERROR: Coverage merge script has syntax errors, falling back to concatenation"
                cat "${COVERAGE_FILES[@]}" > coverage/merged/lcov.info
              else
                # Merge coverage reports properly by file instead of concatenating
                echo "Merging coverage reports..."
                if ! python3 scripts/merge-coverage.py "${COVERAGE_FILES[@]}" -o coverage/merged/lcov.info; then
                  echo "ERROR: Coverage merge failed, falling back to concatenation"
                  cat "${COVERAGE_FILES[@]}" > coverage/merged/lcov.info
                fi
              fi
            fi
            
            # Validate merged file
            if [ ! -s coverage/merged/lcov.info ]; then
              echo "ERROR: Merged coverage file is empty"
              echo '{"total":{"lines":{"pct":0},"statements":{"pct":0},"functions":{"pct":0},"branches":{"pct":0}}}' > coverage-summary.json
              echo "coverage_pct=0" >> $GITHUB_OUTPUT
              echo "status=error" >> $GITHUB_OUTPUT
              exit 1
            fi
            
            echo "Proper file-by-file merge completed with $(wc -l < coverage/merged/lcov.info) lines"
            
            if [ -f coverage/merged/lcov.info ]; then
              echo "Successfully merged coverage reports"
              
              # Check merged coverage file
              echo "Merged coverage file size: $(wc -l < coverage/merged/lcov.info) lines"
              
              # Extract coverage directly from lcov file
              echo "Extracting coverage from merged lcov.info..."
              
              # Extract with error checking
              LINES_FOUND=$(grep -E "^LF:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
              LINES_HIT=$(grep -E "^LH:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
              FUNCTIONS_FOUND=$(grep -E "^FNF:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
              FUNCTIONS_HIT=$(grep -E "^FNH:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
              BRANCHES_FOUND=$(grep -E "^BRF:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
              BRANCHES_HIT=$(grep -E "^BRH:" coverage/merged/lcov.info | awk -F: '{sum+=$2} END {print sum+0}' || echo "0")
              
              echo "Coverage totals: Lines $LINES_HIT/$LINES_FOUND, Functions $FUNCTIONS_HIT/$FUNCTIONS_FOUND, Branches $BRANCHES_HIT/$BRANCHES_FOUND"
              
              # Calculate percentages with robust error handling
              LINES_PCT=$(awk -v hit="$LINES_HIT" -v found="$LINES_FOUND" 'BEGIN {
                if (found == 0 || hit == "" || found == "") print "0.00"
                else printf "%.2f", hit * 100 / found
              }' 2>/dev/null || echo "0.00")
              
              FUNCTIONS_PCT=$(awk -v hit="$FUNCTIONS_HIT" -v found="$FUNCTIONS_FOUND" 'BEGIN {
                if (found == 0 || hit == "" || found == "") print "0.00"
                else printf "%.2f", hit * 100 / found
              }' 2>/dev/null || echo "0.00")
              
              BRANCHES_PCT=$(awk -v hit="$BRANCHES_HIT" -v found="$BRANCHES_FOUND" 'BEGIN {
                if (found == 0 || hit == "" || found == "") print "0.00"
                else printf "%.2f", hit * 100 / found
              }' 2>/dev/null || echo "0.00")
              
              echo "Calculated coverage: Lines ${LINES_PCT}%, Functions ${FUNCTIONS_PCT}%, Branches ${BRANCHES_PCT}%"
              
              # Create coverage-summary.json using echo to avoid heredoc issues
              echo "{
                \"total\": {
                  \"lines\": {
                    \"total\": ${LINES_FOUND},
                    \"covered\": ${LINES_HIT},
                    \"skipped\": 0,
                    \"pct\": ${LINES_PCT}
                  },
                  \"statements\": {
                    \"total\": ${LINES_FOUND},
                    \"covered\": ${LINES_HIT},
                    \"skipped\": 0,
                    \"pct\": ${LINES_PCT}
                  },
                  \"functions\": {
                    \"total\": ${FUNCTIONS_FOUND},
                    \"covered\": ${FUNCTIONS_HIT},
                    \"skipped\": 0,
                    \"pct\": ${FUNCTIONS_PCT}
                  },
                  \"branches\": {
                    \"total\": ${BRANCHES_FOUND},
                    \"covered\": ${BRANCHES_HIT},
                    \"skipped\": 0,
                    \"pct\": ${BRANCHES_PCT}
                  }
                }
              }" > coverage-summary.json
              
              echo "Generated coverage-summary.json with ${LINES_PCT}% line coverage"
              echo "coverage_pct=${LINES_PCT}" >> $GITHUB_OUTPUT
              echo "status=available" >> $GITHUB_OUTPUT
            else
              echo "Failed to merge coverage reports"
              echo '{"total":{"lines":{"pct":0},"statements":{"pct":0},"functions":{"pct":0},"branches":{"pct":0}}}' > coverage-summary.json
              echo "coverage_pct=0" >> $GITHUB_OUTPUT
              echo "status=missing" >> $GITHUB_OUTPUT
            fi
          else
            echo "No coverage files found to merge."
            echo '{"total":{"lines":{"pct":0},"statements":{"pct":0},"functions":{"pct":0},"branches":{"pct":0}}}' > coverage-summary.json
            echo "coverage_pct=0" >> $GITHUB_OUTPUT
            echo "status=missing" >> $GITHUB_OUTPUT
          fi

      - name: Generate comprehensive test report
        run: |
          mkdir -p reports
          OVERALL_COVERAGE=$(jq -r '.total.lines.pct // 0' coverage-summary.json 2>/dev/null | awk '{printf "%.2f", $1}' || echo "0.00")
          
          cat > reports/test-summary.md << EOF
          # Test Summary Report
          
          **Generated:** $(date)
          **Commit:** \`${{ github.sha }}\`
          **Branch:** \`${{ github.ref_name }}\`
          **Trigger:** ${{ github.event_name }}
          
          ## 🎯 Overall Merged Coverage: ${OVERALL_COVERAGE}%
          *This is the accumulated coverage from all test suites that ran.*
          
          ---
          
          ## Test Suite Status
          
          | Test Type   | Result  |
          |-------------|---------|
          EOF
          
          # This loop now ONLY reports job status, not confusing individual coverage.
          for test_type in unit integration e2e performance security; do
            job_status=""
            
            case $test_type in
              "unit") job_status="${{ needs.unit-tests.result }}" ;;
              "integration") job_status="${{ needs.integration-tests.result }}" ;;
              "e2e") job_status="${{ needs.e2e-tests.result }}" ;;
              "performance") job_status="${{ needs.performance-tests.result }}" ;;
              "security") job_status="${{ needs.security-tests.result }}" ;;
            esac
            
            # Determine test status based on job result
            if [[ -n "$job_status" && "$job_status" != "skipped" ]]; then
              case "$job_status" in
                "success") test_status="✅ Pass" ;;
                "failure") test_status="❌ Fail" ;;
                "cancelled") test_status="🛑 Cancel" ;;
                *) test_status="⚠️ Unknown" ;;
              esac
              printf "| %-11s | %-7s |\n" "${test_type^}" "$test_status" >> reports/test-summary.md
            fi
          done
          
          echo "" >> reports/test-summary.md
          echo "## Detailed Results" >> reports/test-summary.md
          
          for test_type in unit integration e2e performance security; do
            echo "" >> reports/test-summary.md
            echo "### ${test_type^} Tests" >> reports/test-summary.md
            echo "" >> reports/test-summary.md
            echo "\`\`\`" >> reports/test-summary.md
            if [ "$test_type" == "unit" ]; then
              if [ -f test-results/unit/node18/coverage/unit/test-output-node18.log ]; then
                cat test-results/unit/node18/coverage/unit/test-output-node18.log >> reports/test-summary.md
              else
                echo "No output for Node 18" >> reports/test-summary.md
              fi
              echo "" >> reports/test-summary.md
              echo "---" >> reports/test-summary.md
              echo "" >> reports/test-summary.md
              if [ -f test-results/unit/node20/coverage/unit/test-output-node20.log ]; then
                cat test-results/unit/node20/coverage/unit/test-output-node20.log >> reports/test-summary.md
              else
                echo "No output for Node 20" >> reports/test-summary.md
              fi
            else
              if [ -f test-results/$test_type/coverage/$test_type/test-output.log ]; then
                 cat test-results/$test_type/coverage/$test_type/test-output.log >> reports/test-summary.md
              elif [ -f test-results/$test_type/test-output.log ]; then
                cat test-results/$test_type/test-output.log >> reports/test-summary.md
              else
                echo "No output for $test_type tests" >> reports/test-summary.md
              fi
            fi
            echo "\`\`\`" >> reports/test-summary.md
          done
          
          echo "" >> reports/test-summary.md
          echo "Detailed test results and coverage reports are available in the individual test artifacts:" >> reports/test-summary.md
          echo "- \`unit-test-results-node18\`, \`unit-test-results-node20\`" >> reports/test-summary.md
          echo "- \`integration-test-results\`" >> reports/test-summary.md
          echo "- \`e2e-test-results\`" >> reports/test-summary.md
          echo "- \`performance-test-results\`" >> reports/test-summary.md
          echo "- \`security-test-results\`" >> reports/test-summary.md
          
          # Add overall coverage summary
          if [ -f coverage-summary.json ]; then
            echo "" >> reports/test-summary.md
            echo "## Merged Coverage Breakdown" >> reports/test-summary.md
            echo "" >> reports/test-summary.md
            
            BRANCH_COVERAGE=$(jq -r '.total.branches.pct // 0' coverage-summary.json 2>/dev/null | awk '{printf "%.2f", $1}' || echo "0.00")
            FUNCTION_COVERAGE=$(jq -r '.total.functions.pct // 0' coverage-summary.json 2>/dev/null | awk '{printf "%.2f", $1}' || echo "0.00")
            
            echo "- **Lines:** ${OVERALL_COVERAGE}%" >> reports/test-summary.md
            echo "- **Branches:** ${BRANCH_COVERAGE}%" >> reports/test-summary.md  
            echo "- **Functions:** ${FUNCTION_COVERAGE}%" >> reports/test-summary.md
            echo "" >> reports/test-summary.md
            
            # Coverage quality assessment using integer comparison
            COVERAGE_INT=$(awk -v val="$OVERALL_COVERAGE" 'BEGIN {printf "%.0f", val * 100}')
            if [ "$COVERAGE_INT" -ge 2500 ]; then
              echo "✅ **Coverage Quality:** Meets minimum standards (≥25%)" >> reports/test-summary.md
            elif [ "$COVERAGE_INT" -ge 1500 ]; then
              echo "⚠️ **Coverage Quality:** Below target but acceptable (≥15%)" >> reports/test-summary.md
            else
              echo "❌ **Coverage Quality:** Below minimum standards (<15%)" >> reports/test-summary.md
            fi
          fi

      - name: Validate coverage quality
        run: |
          if [ "${{ steps.coverage.outputs.status }}" == "available" ]; then
            COVERAGE="${{ steps.coverage.outputs.coverage_pct }}"
            # Ensure COVERAGE is a valid number
            COVERAGE=$(echo "$COVERAGE" | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            
            echo "Current coverage: ${COVERAGE}%"
            
            # Set coverage quality gates using integer comparison
            COVERAGE_INT=$(awk -v val="$COVERAGE" 'BEGIN {printf "%.0f", val * 100}')
            if [ "$COVERAGE_INT" -lt 1000 ]; then
              echo "❌ Coverage critically low: $COVERAGE%"
              echo "coverage_status=critical" >> $GITHUB_ENV
            elif [ "$COVERAGE_INT" -lt 1500 ]; then
              echo "⚠️ Coverage below minimum: $COVERAGE%"
              echo "coverage_status=warning" >> $GITHUB_ENV
            elif [ "$COVERAGE_INT" -ge 2500 ]; then
              echo "✅ Coverage meets target: $COVERAGE%"
              echo "coverage_status=good" >> $GITHUB_ENV
            else
              echo "📈 Coverage progressing: $COVERAGE%"
              echo "coverage_status=progress" >> $GITHUB_ENV
            fi
            
            # Create coverage metrics for tracking - Ensure all values are valid numbers
            BRANCHES_PCT=$(jq -r '.total.branches.pct // 0' coverage-summary.json 2>/dev/null | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            FUNCTIONS_PCT=$(jq -r '.total.functions.pct // 0' coverage-summary.json 2>/dev/null | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            STATEMENTS_PCT=$(jq -r '.total.statements.pct // 0' coverage-summary.json 2>/dev/null | awk '{if($1 == "") print "0"; else printf "%.2f", $1}')
            QUALITY_SCORE=$(awk -v cov="$COVERAGE" -v br="$BRANCHES_PCT" -v fn="$FUNCTIONS_PCT" 'BEGIN {printf "%.2f", cov * 0.4 + br * 0.3 + fn * 0.3}')
            
            echo "{
              \"timestamp\": \"$(date -Iseconds)\",
              \"commit\": \"${{ github.sha }}\",
              \"branch\": \"${{ github.ref_name }}\",
              \"coverage\": {
                \"lines\": $COVERAGE,
                \"branches\": $BRANCHES_PCT,
                \"functions\": $FUNCTIONS_PCT,
                \"statements\": $STATEMENTS_PCT
              },
              \"quality_score\": $QUALITY_SCORE
            }" > coverage-metrics.json
          else
            echo "No coverage data available"
            echo "coverage_status=missing" >> $GITHUB_ENV
          fi

      - name: Upload merged coverage to Codecov
        if: steps.coverage.outputs.status == 'available'
        uses: codecov/codecov-action@v4
        with:
          file: coverage/merged/lcov.info
          flags: merged
          name: merged-coverage
          fail_ci_if_error: false
          plugins: ''

      - name: Comment commit with test results
        if: github.event_name == 'push'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            const path = 'reports/test-summary.md';
            
            if (fs.existsSync(path)) {
              try {
                const body = fs.readFileSync(path, 'utf8');
                
                await github.rest.repos.createCommitComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  commit_sha: context.sha,
                  body: body
                });
                console.log('Commit comment created successfully');
              } catch (error) {
                console.log('Failed to create commit comment:', error.message);
              }
            } else {
              console.log('Test summary file not found');
            }

      - name: Store final test report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: |
            reports/
            coverage/merged/
            coverage-summary.json
            coverage-metrics.json
            test-results/
          retention-days: 30

      - name: Update status check
        if: always()
        run: |
          # Check individual job statuses and fail if any critical test failed
          UNIT_STATUS="${{ needs.unit-tests.result }}"
          INTEGRATION_STATUS="${{ needs.integration-tests.result }}"
          E2E_STATUS="${{ needs.e2e-tests.result }}"
          PERFORMANCE_STATUS="${{ needs.performance-tests.result }}"
          SECURITY_STATUS="${{ needs.security-tests.result }}"
          
          echo "Unit tests: $UNIT_STATUS"
          echo "Integration tests: $INTEGRATION_STATUS" 
          echo "E2E tests: $E2E_STATUS"
          echo "Performance tests: $PERFORMANCE_STATUS"
          echo "Security tests: $SECURITY_STATUS"
          
          # Fail if any test job failed (not skipped)
          if [[ "$UNIT_STATUS" == "failure" || "$INTEGRATION_STATUS" == "failure" || "$E2E_STATUS" == "failure" || "$PERFORMANCE_STATUS" == "failure" || "$SECURITY_STATUS" == "failure" ]]; then
            echo "❌ One or more test suites failed"
            exit 1
          elif [[ "$UNIT_STATUS" == "success" || "$INTEGRATION_STATUS" == "success" || "$E2E_STATUS" == "success" || "$PERFORMANCE_STATUS" == "success" || "$SECURITY_STATUS" == "success" ]]; then
            echo "✅ All executed tests completed successfully"
          else
            echo "⚠️ All tests were skipped"
          fi

# Cleanup job to remove old artifacts
  cleanup:
    name: Cleanup Old Artifacts
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    steps:
      - name: Delete old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const cutoff = new Date();
            cutoff.setDate(cutoff.getDate() - 30); // Keep artifacts for 30 days
            
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
            });
            
            for (const artifact of artifacts.data.artifacts) {
              if (new Date(artifact.created_at) < cutoff) {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
                console.log(`Deleted artifact: ${artifact.name}`);
              }
            }